{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuning GPT-2 for Poetry and TensorFlow Lite Conversion",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jordankettles/345-group-project/blob/jordan_trained/gpt2-notebook/Finetuning_GPT_2_for_Poetry_and_TensorFlow_Lite_Conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzxl1vYX-1kk"
      },
      "source": [
        "# Finetuning GPT-2 for Poetry and TensorFlow Lite Conversion\n",
        "##COSC345 2021 Group Project\n",
        "---\n",
        "\n",
        "We are building an AI generated poetry app using a custom trained AI model to generate poems based on short prompts. This notebook is for training and converting OpenAI's GPT-2 (small) model. \n",
        "\n",
        "Uses Hugging Face's GPT-2 Model and Transformers Repo under Apache 2.0 License\n",
        "\n",
        "> Written by Jordan Kettles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAiTzuBwaCXz"
      },
      "source": [
        "# Setup\n",
        "1.   Make sure GPU is enabled, go to edit->notebook settings->Hardware Accelerator GPU\n",
        "2.   Set Encoding\n",
        "3.   Clone Hugging Face's Transformers github repo\n",
        "4.   Mount your Google Drive to save the model. Requires a google account.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC"
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgus89iueaGw"
      },
      "source": [
        "# Preprocessing our dataset\n",
        "\n",
        "\n",
        "1.   Download some poems\n",
        "2.   Format the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMQjSkdweohc",
        "outputId": "9d28b70a-5913-4ee6-9fc2-aa823667ce17"
      },
      "source": [
        "import re\n",
        "!wget https://rawcdn.githack.com/moona740/Nat_Moore_MA_Thesis/ffd9d46fbc034042e26eae25d65f0e98f9418b6c/pf_1.txt\n",
        "f = open(\"pf_1.txt\", \"r\")\n",
        "input = f.read()\n",
        "f.close()\n",
        "regex = re.compile(r'<\\|startoftext\\|>')\n",
        "input = regex.sub(\"\", input)\n",
        "regex = re.compile(r'<\\|endoftext\\|>')\n",
        "input = regex.sub(\"\", input)\n",
        "f2 = open(\"dataset.txt\", \"w\")\n",
        "f2.write(input)\n",
        "f2.close()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-07 07:36:58--  https://rawcdn.githack.com/moona740/Nat_Moore_MA_Thesis/ffd9d46fbc034042e26eae25d65f0e98f9418b6c/pf_1.txt\n",
            "Resolving rawcdn.githack.com (rawcdn.githack.com)... 104.21.234.230, 104.21.234.231, 2606:4700:3038::6815:eae6, ...\n",
            "Connecting to rawcdn.githack.com (rawcdn.githack.com)|104.21.234.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘pf_1.txt’\n",
            "\n",
            "pf_1.txt                [  <=>               ]   1.10M  3.05MB/s    in 0.4s    \n",
            "\n",
            "2021-08-07 07:36:59 (3.05 MB/s) - ‘pf_1.txt’ saved [1149443]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfJ5b3CQXqr"
      },
      "source": [
        "# Training\n",
        "Let's start training!\n",
        "This script automatically downloads the model for us before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEn_ihcGI00T"
      },
      "source": [
        "%cd /content/transformers/examples/pytorch/language-modeling/\n",
        "!pip install -r requirements.txt\n",
        "!python run_clm.py \\\n",
        "--model_name_or_path gpt2 \\\n",
        "--train_file /content/dataset.txt \\\n",
        "--do_train \\\n",
        "--num_train_epochs 3 \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--output_dir /content/test-clm \\\n",
        "--overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLScSKMyDMz7"
      },
      "source": [
        "# Testing (Optional)\n",
        "Let's test the output of the model. Is it poetic enough yet?\n",
        "\n",
        "Change the prompt to see different outputs.\n",
        "\n",
        "You might get a warning about some weights not being used, but you can ignore them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNGP7sjBCsdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223e2e3c-1750-4646-e3f2-f83fac9eaff9"
      },
      "source": [
        "from transformers import pipeline, TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "prompt = \"Moloch whose eye are a thousand blind windows!\"\n",
        "model = TFGPT2LMHeadModel.from_pretrained('/content/test-clm/', from_pt=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/test-clm/', from_pt=True)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "print(generator(prompt, max_length=42, num_return_sequences=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['lm_head.weight', 'transformer.h.4.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias']\n",
            "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Moloch whose eyes are a thousand blind windows! he speaks the same language, but it can be only with our eyes! the eye has no purpose; it cannot, therefore, see or understand, but'}, {'generated_text': 'Moloch whose eyes are a thousand blind windows! it might take forever to hear her; but if you listen for a moment and give your attention, the thought is in vain, i hope, for she'}, {'generated_text': 'Moloch whose eyes are a thousand blind windows! a light that does not turn in a cloud over a night. who knows? not to speak? i fear you will wake againfrom the coldness!'}, {'generated_text': 'Moloch whose eyes are a thousand blind windows! what do your eyes look like? what did the moon say to you? what did the earth say to you? where were we? now where are you'}, {'generated_text': 'Moloch whose eyes are a thousand blind windows! i do not believe that her eyes can see,i do not believe that hers is only a hundred feet on. the only wayi can see her is'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q4tp5EtC-Dr"
      },
      "source": [
        "# TensorFlow Lite Conversion\n",
        "\n",
        "After training the model we can now covert the model to TensorFlow Lite.\n",
        "\n",
        "We specify that the model should be optimized using post-training floating point-16 quantization, which is a fancy way of saying reducing the size by ~50% with minimal loss of quality. \n",
        "\n",
        "You might get a lot of warning messages here too, but that's ok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTYYgzK0C8uW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57417bb2-db39-434a-bcc2-17f47e856d4a"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/\n",
        "!mkdir -p poem-ai/tf-lite/\n",
        "%cd poem-ai/tf-lite/\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained('/content/test-clm/', from_pt=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/test-clm/', from_pt=True)\n",
        "\n",
        "# model = TFGPT2LMHeadModel.from_pretrained('../saved-model/', from_pt=True)\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('../saved-model/', from_pt=True)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "keras_input = tf.keras.Input([64], batch_size=1, dtype=tf.int32)\n",
        "keras_output = model(keras_input, training=False)\n",
        "model = tf.keras.Model(keras_input, keras_output)\n",
        "\n",
        "print(model.inputs)\n",
        "print(model.outputs)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "\n",
        "tflite_fp16_model = converter.convert()\n",
        "open(\"gpt2-f16.tflite\", \"wb\").write(tflite_fp16_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/poem-ai/tf-lite\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['lm_head.weight', 'transformer.h.8.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.9.attn.masked_bias']\n",
            "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tfgp_t2lm_head_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "transformer (TFGPT2MainLayer multiple                  124439808 \n",
            "=================================================================\n",
            "Total params: 124,439,808\n",
            "Trainable params: 124,439,808\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa7bca95d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa7bca95d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fa7d7c45170> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fa7d7c45170> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "[<KerasTensor: shape=(1, 64) dtype=int32 (created by layer 'input_1')>]\n",
            "[<KerasTensor: shape=(1, 64, 50257) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>, <KerasTensor: shape=(2, 1, 12, 64, 64) dtype=float32 (created by layer 'tfgp_t2lm_head_model')>]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as wte_layer_call_fn, wte_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, ln_f_layer_call_fn while saving (showing 5 of 735). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpa_s61rbl/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpa_s61rbl/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "INFO:absl:Using new converter: If you encounter a problem please file a bug. You can opt-out by setting experimental_new_converter=False\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247868144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}